{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_3bXwvQhnU6",
        "outputId": "90e42d18-748f-43ee-9371-6fbeac347159"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==2.15.0 (from versions: 2.16.0rc0, 2.16.1, 2.16.2, 2.17.0rc0, 2.17.0rc1, 2.17.0, 2.17.1, 2.18.0rc0, 2.18.0rc1, 2.18.0rc2, 2.18.0, 2.18.1, 2.19.0rc0, 2.19.0, 2.19.1, 2.20.0rc0, 2.20.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow==2.15.0\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (5.24.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly) (9.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: keras-tuner in /usr/local/lib/python3.12/dist-packages (1.4.8)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.12/dist-packages (from keras-tuner) (3.10.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from keras-tuner) (25.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from keras-tuner) (2.32.4)\n",
            "Requirement already satisfied: kt-legacy in /usr/local/lib/python3.12/dist-packages (from keras-tuner) (1.0.5)\n",
            "Requirement already satisfied: grpcio in /usr/local/lib/python3.12/dist-packages (from keras-tuner) (1.76.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from keras-tuner) (5.29.5)\n",
            "Requirement already satisfied: typing-extensions~=4.12 in /usr/local/lib/python3.12/dist-packages (from grpcio->keras-tuner) (4.15.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from keras->keras-tuner) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from keras->keras-tuner) (2.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras->keras-tuner) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras->keras-tuner) (0.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.12/dist-packages (from keras->keras-tuner) (3.15.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras->keras-tuner) (0.18.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.12/dist-packages (from keras->keras-tuner) (0.5.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->keras-tuner) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->keras-tuner) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->keras-tuner) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->keras-tuner) (2025.11.12)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras->keras-tuner) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras->keras-tuner) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras->keras-tuner) (0.1.2)\n",
            "‚úÖ All dependencies installed successfully!\n"
          ]
        }
      ],
      "source": [
        "# %% [markdown]\n",
        "# ## üìã Step 1: Install Dependencies\n",
        "\n",
        "# %%\n",
        "!pip install tensorflow==2.15.0\n",
        "!pip install scikit-learn pandas numpy matplotlib seaborn plotly\n",
        "!pip install keras-tuner\n",
        "print(\"‚úÖ All dependencies installed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6wAuMgphvZS",
        "outputId": "40de274c-7627-4ad5-f7a0-22b065a02955"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ GPU Available: 1 GPU(s)\n",
            "TensorFlow version: 2.19.0\n",
            "Keras version: 3.10.0\n"
          ]
        }
      ],
      "source": [
        "# %% [markdown]\n",
        "# ## üì¶ Step 2: Import Libraries\n",
        "\n",
        "# %%\n",
        "import os\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import pickle\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# TensorFlow and Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models, callbacks\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import Huber\n",
        "from tensorflow.keras.metrics import MeanAbsoluteError, RootMeanSquaredError\n",
        "\n",
        "# Scikit-learn\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Plotly for interactive visualizations\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# GPU Configuration\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        print(f\"‚úÖ GPU Available: {len(gpus)} GPU(s)\")\n",
        "    except RuntimeError as e:\n",
        "        print(e)\n",
        "else:\n",
        "    print(\"‚ÑπÔ∏è Running on CPU\")\n",
        "\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"Keras version: {keras.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "Ujc5AxL5hvok",
        "outputId": "7b1a5360-ceb5-4cbc-fc93-ad8d40ff2133"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Starting dataset upload process...\n",
            "\n",
            "üì§ Upload your CMAPSS dataset files\n",
            "======================================================================\n",
            "Required files:\n",
            "  - train_FD001.txt, test_FD001.txt, RUL_FD001.txt\n",
            "  - train_FD002.txt, test_FD002.txt, RUL_FD002.txt\n",
            "  - train_FD003.txt, test_FD003.txt, RUL_FD003.txt\n",
            "  - train_FD004.txt, test_FD004.txt, RUL_FD004.txt\n",
            "======================================================================\n",
            "\n",
            "üí° You can upload a ZIP file or individual files\n",
            "Click 'Choose Files' below:\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d5577408-e1f9-48ed-8a6a-79889431023e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d5577408-e1f9-48ed-8a6a-79889431023e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "TypeError",
          "evalue": "'NoneType' object is not subscriptable",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1235329440.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;31m# Execute upload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"üöÄ Starting dataset upload process...\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m \u001b[0mavailable_datasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupload_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1235329440.py\u001b[0m in \u001b[0;36mupload_datasets\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Click 'Choose Files' below:\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0muploaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# Create data directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m(target_dir)\u001b[0m\n\u001b[1;32m     70\u001b[0m   \"\"\"\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m   \u001b[0muploaded_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m   \u001b[0mlocal_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36m_upload_files\u001b[0;34m(multiple)\u001b[0m\n\u001b[1;32m    169\u001b[0m   \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_collections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m   \u001b[0;32mwhile\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'action'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'complete'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m     result = _output.eval_js(\n\u001b[1;32m    173\u001b[0m         'google.colab._files._uploadFilesContinue(\"{output_id}\")'.format(\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
          ]
        }
      ],
      "source": [
        "# %% [markdown]\n",
        "# ## üìÅ Step 3: Manual Dataset Upload\n",
        "\n",
        "# %%\n",
        "from google.colab import files\n",
        "import zipfile\n",
        "\n",
        "def upload_datasets():\n",
        "    \"\"\"Upload and organize CMAPSS datasets\"\"\"\n",
        "    print(\"üì§ Upload your CMAPSS dataset files\")\n",
        "    print(\"=\" * 70)\n",
        "    print(\"Required files:\")\n",
        "    print(\"  - train_FD001.txt, test_FD001.txt, RUL_FD001.txt\")\n",
        "    print(\"  - train_FD002.txt, test_FD002.txt, RUL_FD002.txt\")\n",
        "    print(\"  - train_FD003.txt, test_FD003.txt, RUL_FD003.txt\")\n",
        "    print(\"  - train_FD004.txt, test_FD004.txt, RUL_FD004.txt\")\n",
        "    print(\"=\" * 70)\n",
        "    print(\"\\nüí° You can upload a ZIP file or individual files\")\n",
        "    print(\"Click 'Choose Files' below:\\n\")\n",
        "\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    # Create data directory\n",
        "    data_dir = Path('./data')\n",
        "    data_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    # Process uploaded files\n",
        "    for filename, content in uploaded.items():\n",
        "        filepath = Path(filename)\n",
        "\n",
        "        # Handle ZIP files\n",
        "        if filepath.suffix == '.zip':\n",
        "            print(f\"\\nüì¶ Extracting {filename}...\")\n",
        "            with open(filename, 'wb') as f:\n",
        "                f.write(content)\n",
        "\n",
        "            with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
        "                zip_ref.extractall(data_dir)\n",
        "\n",
        "            os.remove(filename)\n",
        "            print(f\"‚úì Extracted to {data_dir}\")\n",
        "        else:\n",
        "            # Save individual files\n",
        "            file_path = data_dir / filename\n",
        "            with open(file_path, 'wb') as f:\n",
        "                f.write(content)\n",
        "            print(f\"‚úì Saved {filename} to {data_dir}\")\n",
        "\n",
        "    # Verify uploaded files\n",
        "    print(\"\\nüìã Verifying dataset files...\")\n",
        "    datasets = ['FD001', 'FD002', 'FD003', 'FD004']\n",
        "    available_datasets = []\n",
        "\n",
        "    for dataset in datasets:\n",
        "        train_file = data_dir / f'train_{dataset}.txt'\n",
        "        test_file = data_dir / f'test_{dataset}.txt'\n",
        "        rul_file = data_dir / f'RUL_{dataset}.txt'\n",
        "\n",
        "        if train_file.exists() and test_file.exists() and rul_file.exists():\n",
        "            available_datasets.append(dataset)\n",
        "            print(f\"‚úÖ {dataset}: Complete\")\n",
        "        else:\n",
        "            missing = []\n",
        "            if not train_file.exists(): missing.append(f\"train_{dataset}.txt\")\n",
        "            if not test_file.exists(): missing.append(f\"test_{dataset}.txt\")\n",
        "            if not rul_file.exists(): missing.append(f\"RUL_{dataset}.txt\")\n",
        "            print(f\"‚ö†Ô∏è {dataset}: Missing {', '.join(missing)}\")\n",
        "\n",
        "    if available_datasets:\n",
        "        print(f\"\\n‚úÖ Successfully loaded {len(available_datasets)} dataset(s): {', '.join(available_datasets)}\")\n",
        "    else:\n",
        "        print(\"\\n‚ùå No complete datasets found. Please upload the required files.\")\n",
        "\n",
        "    return available_datasets\n",
        "\n",
        "# Execute upload\n",
        "print(\"üöÄ Starting dataset upload process...\\n\")\n",
        "available_datasets = upload_datasets()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "sUwPT5LqhvxG"
      },
      "outputs": [],
      "source": [
        "# %% [markdown]\n",
        "# ## üîß Step 4: Configuration and Dataset Handler\n",
        "\n",
        "# %%\n",
        "class CMAPSSConfig:\n",
        "    \"\"\"Configuration for CMAPSS datasets\"\"\"\n",
        "\n",
        "    CONFIG = {\n",
        "        'FD001': {\n",
        "            'train_path': 'train_FD001.txt',\n",
        "            'test_path': 'test_FD001.txt',\n",
        "            'rul_path': 'RUL_FD001.txt',\n",
        "            'seq_len': 50,  # Optimized sequence length\n",
        "            'rul_clip': 125,\n",
        "            'batch_size': 512,\n",
        "            'sensor_cols': [2, 3, 4, 7, 8, 9, 11, 12, 13, 14, 15, 17, 20, 21],\n",
        "            'op_cols': [1, 2, 3],\n",
        "        },\n",
        "        'FD002': {\n",
        "            'train_path': 'train_FD002.txt',\n",
        "            'test_path': 'test_FD002.txt',\n",
        "            'rul_path': 'RUL_FD002.txt',\n",
        "            'seq_len': 30,\n",
        "            'rul_clip': 125,\n",
        "            'batch_size': 512,\n",
        "            'sensor_cols': [2, 3, 4, 7, 8, 9, 11, 12, 13, 14, 15, 17, 20, 21],\n",
        "            'op_cols': [1, 2, 3],\n",
        "        },\n",
        "        'FD003': {\n",
        "            'train_path': 'train_FD003.txt',\n",
        "            'test_path': 'test_FD003.txt',\n",
        "            'rul_path': 'RUL_FD003.txt',\n",
        "            'seq_len': 50,\n",
        "            'rul_clip': 125,\n",
        "            'batch_size': 512,\n",
        "            'sensor_cols': [2, 3, 4, 7, 8, 9, 11, 12, 13, 14, 15, 17, 20, 21],\n",
        "            'op_cols': [1, 2, 3],\n",
        "        },\n",
        "        'FD004': {\n",
        "            'train_path': 'train_FD004.txt',\n",
        "            'test_path': 'test_FD004.txt',\n",
        "            'rul_path': 'RUL_FD004.txt',\n",
        "            'seq_len': 30,\n",
        "            'rul_clip': 125,\n",
        "            'batch_size': 512,\n",
        "            'sensor_cols': [2, 3, 4, 7, 8, 9, 11, 12, 13, 14, 15, 17, 20, 21],\n",
        "            'op_cols': [1, 2, 3],\n",
        "        }\n",
        "    }\n",
        "\n",
        "    @staticmethod\n",
        "    def get_feature_names():\n",
        "        \"\"\"Get column names for CMAPSS data\"\"\"\n",
        "        columns = ['unit', 'cycle']\n",
        "        columns += [f'op_setting_{i}' for i in range(1, 4)]\n",
        "        columns += [f'sensor_{i}' for i in range(1, 22)]\n",
        "        return columns\n",
        "\n",
        "class DataProcessor:\n",
        "    \"\"\"Process CMAPSS datasets\"\"\"\n",
        "\n",
        "    def __init__(self, dataset_name, base_path='./data'):\n",
        "        self.dataset_name = dataset_name\n",
        "        self.config = CMAPSSConfig.CONFIG[dataset_name]\n",
        "        self.base_path = Path(base_path)\n",
        "        self.feature_names = CMAPSSConfig.get_feature_names()\n",
        "        self.scaler = StandardScaler()\n",
        "\n",
        "    def load_data(self):\n",
        "        \"\"\"Load training, test, and RUL data\"\"\"\n",
        "        train_path = self.base_path / self.config['train_path']\n",
        "        test_path = self.base_path / self.config['test_path']\n",
        "        rul_path = self.base_path / self.config['rul_path']\n",
        "\n",
        "        # Load data\n",
        "        train_df = pd.read_csv(train_path, sep=r'\\s+', header=None, names=self.feature_names)\n",
        "        test_df = pd.read_csv(test_path, sep=r'\\s+', header=None, names=self.feature_names)\n",
        "        rul_df = pd.read_csv(rul_path, sep=r'\\s+', header=None, names=['RUL'])\n",
        "\n",
        "        print(f\"\\nüìä Dataset: {self.dataset_name}\")\n",
        "        print(f\"   Training samples: {len(train_df):,}\")\n",
        "        print(f\"   Test samples: {len(test_df):,}\")\n",
        "        print(f\"   Training units: {train_df['unit'].nunique()}\")\n",
        "        print(f\"   Test units: {test_df['unit'].nunique()}\")\n",
        "\n",
        "        return train_df, test_df, rul_df\n",
        "\n",
        "    def add_features(self, df):\n",
        "        \"\"\"Add engineered features\"\"\"\n",
        "        # Rolling statistics\n",
        "        sensor_cols = [f'sensor_{i}' for i in self.config['sensor_cols']]\n",
        "\n",
        "        for col in sensor_cols:\n",
        "            # Rolling mean (3 cycles)\n",
        "            df[f'{col}_rolling_mean'] = df.groupby('unit')[col].transform(\n",
        "                lambda x: x.rolling(window=3, min_periods=1).mean()\n",
        "            )\n",
        "            # Rolling std (3 cycles)\n",
        "            df[f'{col}_rolling_std'] = df.groupby('unit')[col].transform(\n",
        "                lambda x: x.rolling(window=3, min_periods=1).std().fillna(0)\n",
        "            )\n",
        "\n",
        "        return df\n",
        "\n",
        "    def compute_rul(self, df):\n",
        "        \"\"\"Compute Remaining Useful Life\"\"\"\n",
        "        max_cycle = df.groupby('unit')['cycle'].max().reset_index()\n",
        "        max_cycle.columns = ['unit', 'max_cycle']\n",
        "\n",
        "        df = df.merge(max_cycle, on='unit', how='left')\n",
        "        df['RUL'] = df['max_cycle'] - df['cycle']\n",
        "        df['RUL'] = df['RUL'].clip(upper=self.config['rul_clip'])\n",
        "        df.drop('max_cycle', axis=1, inplace=True)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def select_features(self, df):\n",
        "        \"\"\"Select relevant features\"\"\"\n",
        "        sensor_cols = [f'sensor_{i}' for i in self.config['sensor_cols']]\n",
        "        op_cols = [f'op_setting_{i}' for i in self.config['op_cols']]\n",
        "\n",
        "        # Add engineered features\n",
        "        rolling_cols = [col for col in df.columns if 'rolling' in col]\n",
        "\n",
        "        feature_cols = op_cols + sensor_cols + rolling_cols\n",
        "\n",
        "        base_cols = ['unit', 'cycle']\n",
        "        if 'RUL' in df.columns:\n",
        "            return df[base_cols + feature_cols + ['RUL']]\n",
        "        return df[base_cols + feature_cols]\n",
        "\n",
        "    def normalize_features(self, train_df, test_df):\n",
        "        \"\"\"Normalize features using StandardScaler\"\"\"\n",
        "        feature_cols = [col for col in train_df.columns\n",
        "                       if col not in ['unit', 'cycle', 'RUL']]\n",
        "\n",
        "        # Fit on training data\n",
        "        train_df[feature_cols] = self.scaler.fit_transform(train_df[feature_cols])\n",
        "\n",
        "        # Transform test data\n",
        "        test_df[feature_cols] = self.scaler.transform(test_df[feature_cols])\n",
        "\n",
        "        return train_df, test_df, feature_cols\n",
        "\n",
        "    def create_sequences(self, df, is_test=False):\n",
        "        \"\"\"Create sequences for LSTM/GRU models\"\"\"\n",
        "        seq_len = self.config['seq_len']\n",
        "        sequences = []\n",
        "        labels = []\n",
        "\n",
        "        for unit in df['unit'].unique():\n",
        "            unit_data = df[df['unit'] == unit].sort_values('cycle')\n",
        "\n",
        "            # Get features and labels\n",
        "            feature_cols = [col for col in unit_data.columns\n",
        "                          if col not in ['unit', 'cycle', 'RUL']]\n",
        "            features = unit_data[feature_cols].values\n",
        "\n",
        "            if is_test:\n",
        "                # For test: take last sequence\n",
        "                if len(features) >= seq_len:\n",
        "                    seq = features[-seq_len:]\n",
        "                else:\n",
        "                    # Pad if too short\n",
        "                    pad_len = seq_len - len(features)\n",
        "                    pad = np.repeat(features[0:1], pad_len, axis=0)\n",
        "                    seq = np.vstack([pad, features])\n",
        "                sequences.append(seq)\n",
        "            else:\n",
        "                # For training: create overlapping windows\n",
        "                rul_values = unit_data['RUL'].values\n",
        "\n",
        "                for i in range(len(features) - seq_len + 1):\n",
        "                    sequences.append(features[i:i+seq_len])\n",
        "                    labels.append(rul_values[i+seq_len-1])\n",
        "\n",
        "        sequences = np.array(sequences, dtype=np.float32)\n",
        "\n",
        "        if is_test:\n",
        "            return sequences\n",
        "        else:\n",
        "            labels = np.array(labels, dtype=np.float32)\n",
        "            return sequences, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TTlpXW99hv4s"
      },
      "outputs": [],
      "source": [
        "# %% [markdown]\n",
        "# ## ü§ñ Step 5: Model Architectures\n",
        "\n",
        "# %%\n",
        "def create_lstm_model(input_shape, model_config=None):\n",
        "    \"\"\"Create LSTM model with attention\"\"\"\n",
        "    if model_config is None:\n",
        "        model_config = {\n",
        "            'lstm_units': [128, 64],\n",
        "            'dropout': 0.3,\n",
        "            'dense_units': [64, 32],\n",
        "            'learning_rate': 0.001\n",
        "        }\n",
        "\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "    # LSTM layers\n",
        "    x = layers.LSTM(model_config['lstm_units'][0],\n",
        "                    return_sequences=True)(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(model_config['dropout'])(x)\n",
        "\n",
        "    x = layers.LSTM(model_config['lstm_units'][1],\n",
        "                    return_sequences=True)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(model_config['dropout'])(x)\n",
        "\n",
        "    # Attention mechanism\n",
        "    attention = layers.Dense(1, activation='tanh')(x)\n",
        "    attention = layers.Flatten()(attention)\n",
        "    attention = layers.Activation('softmax')(attention)\n",
        "    attention = layers.RepeatVector(model_config['lstm_units'][1])(attention)\n",
        "    attention = layers.Permute([2, 1])(attention)\n",
        "\n",
        "    # Apply attention\n",
        "    x = layers.Multiply()([x, attention])\n",
        "    x = layers.Lambda(lambda xin: tf.reduce_sum(xin, axis=1))(x)\n",
        "\n",
        "    # Dense layers\n",
        "    for units in model_config['dense_units']:\n",
        "        x = layers.Dense(units, activation='relu')(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Dropout(model_config['dropout'])(x)\n",
        "\n",
        "    # Output\n",
        "    outputs = layers.Dense(1, activation='linear')(x)\n",
        "\n",
        "    model = models.Model(inputs=inputs, outputs=outputs, name='LSTM_Attention')\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=model_config['learning_rate']),\n",
        "        loss=Huber(delta=1.0),\n",
        "        metrics=[RootMeanSquaredError(name='rmse'),\n",
        "                MeanAbsoluteError(name='mae')]\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "def create_gru_model(input_shape, model_config=None):\n",
        "    \"\"\"Create GRU model\"\"\"\n",
        "    if model_config is None:\n",
        "        model_config = {\n",
        "            'gru_units': [128, 64],\n",
        "            'dropout': 0.3,\n",
        "            'dense_units': [64, 32],\n",
        "            'learning_rate': 0.001\n",
        "        }\n",
        "\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "    # GRU layers\n",
        "    x = layers.GRU(model_config['gru_units'][0],\n",
        "                   return_sequences=True)(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(model_config['dropout'])(x)\n",
        "\n",
        "    x = layers.GRU(model_config['gru_units'][1],\n",
        "                   return_sequences=False)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(model_config['dropout'])(x)\n",
        "\n",
        "    # Dense layers\n",
        "    for units in model_config['dense_units']:\n",
        "        x = layers.Dense(units, activation='relu')(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Dropout(model_config['dropout'])(x)\n",
        "\n",
        "    # Output\n",
        "    outputs = layers.Dense(1, activation='linear')(x)\n",
        "\n",
        "    model = models.Model(inputs=inputs, outputs=outputs, name='GRU_Model')\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=model_config['learning_rate']),\n",
        "        loss=Huber(delta=1.0),\n",
        "        metrics=[RootMeanSquaredError(name='rmse'),\n",
        "                MeanAbsoluteError(name='mae')]\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "def create_cnn_lstm_model(input_shape, model_config=None):\n",
        "    \"\"\"Create hybrid CNN-LSTM model\"\"\"\n",
        "    if model_config is None:\n",
        "        model_config = {\n",
        "            'conv_filters': [64, 32],\n",
        "            'lstm_units': [100, 50],\n",
        "            'dropout': 0.3,\n",
        "            'dense_units': [50],\n",
        "            'learning_rate': 0.001\n",
        "        }\n",
        "\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "    # CNN layers for feature extraction\n",
        "    x = layers.Conv1D(filters=model_config['conv_filters'][0],\n",
        "                      kernel_size=3, padding='same', activation='relu')(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
        "    x = layers.Dropout(model_config['dropout'])(x)\n",
        "\n",
        "    x = layers.Conv1D(filters=model_config['conv_filters'][1],\n",
        "                      kernel_size=3, padding='same', activation='relu')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(model_config['dropout'])(x)\n",
        "\n",
        "    # LSTM layers for temporal dependencies\n",
        "    x = layers.LSTM(model_config['lstm_units'][0],\n",
        "                    return_sequences=True)(x)\n",
        "    x = layers.Dropout(model_config['dropout'])(x)\n",
        "\n",
        "    x = layers.LSTM(model_config['lstm_units'][1],\n",
        "                    return_sequences=False)(x)\n",
        "    x = layers.Dropout(model_config['dropout'])(x)\n",
        "\n",
        "    # Dense layers\n",
        "    for units in model_config['dense_units']:\n",
        "        x = layers.Dense(units, activation='relu')(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Dropout(model_config['dropout'])(x)\n",
        "\n",
        "    # Output\n",
        "    outputs = layers.Dense(1, activation='linear')(x)\n",
        "\n",
        "    model = models.Model(inputs=inputs, outputs=outputs, name='CNN_LSTM')\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=model_config['learning_rate']),\n",
        "        loss=Huber(delta=1.0),\n",
        "        metrics=[RootMeanSquaredError(name='rmse'),\n",
        "                MeanAbsoluteError(name='mae')]\n",
        "    )\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SDXUJkRKhv70"
      },
      "outputs": [],
      "source": [
        "# %% [markdown]\n",
        "# ## üìà Step 6: Training Pipeline\n",
        "\n",
        "# %%\n",
        "class ModelTrainer:\n",
        "    \"\"\"Complete training pipeline\"\"\"\n",
        "\n",
        "    def __init__(self, model, dataset_name, model_type):\n",
        "        self.model = model\n",
        "        self.dataset_name = dataset_name\n",
        "        self.model_type = model_type\n",
        "        self.history = None\n",
        "\n",
        "        # Create directories\n",
        "        self.save_dir = Path(f'./models/{dataset_name}_{model_type}_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}')\n",
        "        self.save_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        print(f\"üìÅ Save directory: {self.save_dir}\")\n",
        "\n",
        "    def get_callbacks(self):\n",
        "        \"\"\"Get training callbacks\"\"\"\n",
        "        callbacks_list = [\n",
        "            # Model checkpoint\n",
        "            keras.callbacks.ModelCheckpoint(\n",
        "                filepath=str(self.save_dir / 'best_model.keras'),\n",
        "                monitor='val_loss',\n",
        "                save_best_only=True,\n",
        "                mode='min',\n",
        "                verbose=1\n",
        "            ),\n",
        "\n",
        "            # Early stopping\n",
        "            keras.callbacks.EarlyStopping(\n",
        "                monitor='val_loss',\n",
        "                patience=20,\n",
        "                restore_best_weights=True,\n",
        "                verbose=1\n",
        "            ),\n",
        "\n",
        "            # Reduce learning rate\n",
        "            keras.callbacks.ReduceLROnPlateau(\n",
        "                monitor='val_loss',\n",
        "                factor=0.5,\n",
        "                patience=10,\n",
        "                min_lr=1e-7,\n",
        "                verbose=1\n",
        "            ),\n",
        "\n",
        "            # CSV logger\n",
        "            keras.callbacks.CSVLogger(\n",
        "                filename=str(self.save_dir / 'training_log.csv')\n",
        "            ),\n",
        "\n",
        "            # TensorBoard\n",
        "            keras.callbacks.TensorBoard(\n",
        "                log_dir=str(self.save_dir / 'logs'),\n",
        "                histogram_freq=0\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        return callbacks_list\n",
        "\n",
        "    def train(self, X_train, y_train, X_val, y_val, epochs=150, batch_size=512):\n",
        "        \"\"\"Train the model\"\"\"\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"üéØ Training {self.model_type} model for {self.dataset_name}\")\n",
        "        print(f\"{'='*70}\")\n",
        "        print(f\"Training samples: {len(X_train):,}\")\n",
        "        print(f\"Validation samples: {len(X_val):,}\")\n",
        "        print(f\"Batch size: {batch_size}\")\n",
        "        print(f\"Epochs: {epochs}\")\n",
        "        print(f\"{'='*70}\\n\")\n",
        "\n",
        "        self.history = self.model.fit(\n",
        "            X_train, y_train,\n",
        "            validation_data=(X_val, y_val),\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            callbacks=self.get_callbacks(),\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        # Save final model\n",
        "        self.model.save(self.save_dir / 'final_model.keras')\n",
        "        print(f\"\\n‚úÖ Training complete! Models saved to {self.save_dir}\")\n",
        "\n",
        "        return self.history\n",
        "\n",
        "    def plot_history(self):\n",
        "        \"\"\"Plot training history\"\"\"\n",
        "        if self.history is None:\n",
        "            print(\"No training history available\")\n",
        "            return\n",
        "\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
        "\n",
        "        # Loss\n",
        "        axes[0, 0].plot(self.history.history['loss'], label='Train Loss', linewidth=2)\n",
        "        axes[0, 0].plot(self.history.history['val_loss'], label='Val Loss', linewidth=2)\n",
        "        axes[0, 0].set_title(f'{self.dataset_name} - Loss', fontsize=14, fontweight='bold')\n",
        "        axes[0, 0].set_xlabel('Epoch')\n",
        "        axes[0, 0].set_ylabel('Loss')\n",
        "        axes[0, 0].legend()\n",
        "        axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "        # RMSE\n",
        "        axes[0, 1].plot(self.history.history['rmse'], label='Train RMSE', linewidth=2)\n",
        "        axes[0, 1].plot(self.history.history['val_rmse'], label='Val RMSE', linewidth=2)\n",
        "        axes[0, 1].set_title(f'{self.dataset_name} - RMSE', fontsize=14, fontweight='bold')\n",
        "        axes[0, 1].set_xlabel('Epoch')\n",
        "        axes[0, 1].set_ylabel('RMSE')\n",
        "        axes[0, 1].legend()\n",
        "        axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "        # MAE\n",
        "        axes[1, 0].plot(self.history.history['mae'], label='Train MAE', linewidth=2)\n",
        "        axes[1, 0].plot(self.history.history['val_mae'], label='Val MAE', linewidth=2)\n",
        "        axes[1, 0].set_title(f'{self.dataset_name} - MAE', fontsize=14, fontweight='bold')\n",
        "        axes[1, 0].set_xlabel('Epoch')\n",
        "        axes[1, 0].set_ylabel('MAE')\n",
        "        axes[1, 0].legend()\n",
        "        axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "        # Learning rate\n",
        "        if 'lr' in self.history.history:\n",
        "            axes[1, 1].plot(self.history.history['lr'], color='green', linewidth=2)\n",
        "            axes[1, 1].set_title('Learning Rate', fontsize=14, fontweight='bold')\n",
        "            axes[1, 1].set_xlabel('Epoch')\n",
        "            axes[1, 1].set_ylabel('Learning Rate')\n",
        "            axes[1, 1].set_yscale('log')\n",
        "            axes[1, 1].grid(True, alpha=0.3)\n",
        "        else:\n",
        "            axes[1, 1].axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(self.save_dir / 'training_history.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "BwagkLj0hwGa"
      },
      "outputs": [],
      "source": [
        "# %% [markdown]\n",
        "# ## üìä Step 7: Evaluation and Visualization\n",
        "\n",
        "# %%\n",
        "class ModelEvaluator:\n",
        "    \"\"\"Evaluate and visualize model performance\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def evaluate_model(model, X_test, y_test):\n",
        "        \"\"\"Calculate evaluation metrics\"\"\"\n",
        "        y_pred = model.predict(X_test, verbose=0).flatten()\n",
        "\n",
        "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "        mae = mean_absolute_error(y_test, y_pred)\n",
        "        r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "        # Error distribution\n",
        "        errors = y_pred - y_test\n",
        "        mean_error = np.mean(errors)\n",
        "        std_error = np.std(errors)\n",
        "\n",
        "        metrics = {\n",
        "            'rmse': rmse,\n",
        "            'mae': mae,\n",
        "            'r2': r2,\n",
        "            'mean_error': mean_error,\n",
        "            'std_error': std_error,\n",
        "            'predictions': y_pred,\n",
        "            'actuals': y_test\n",
        "        }\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    @staticmethod\n",
        "    def print_metrics(metrics, dataset_name):\n",
        "        \"\"\"Print evaluation metrics\"\"\"\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"üìä Evaluation Results for {dataset_name}\")\n",
        "        print(f\"{'='*70}\")\n",
        "        print(f\"RMSE:        {metrics['rmse']:.4f} cycles\")\n",
        "        print(f\"MAE:         {metrics['mae']:.4f} cycles\")\n",
        "        print(f\"R¬≤ Score:    {metrics['r2']:.4f}\")\n",
        "        print(f\"Mean Error:  {metrics['mean_error']:.4f} cycles\")\n",
        "        print(f\"Std Error:   {metrics['std_error']:.4f} cycles\")\n",
        "        print(f\"{'='*70}\\n\")\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_predictions(metrics, dataset_name, save_path=None):\n",
        "        \"\"\"Create comprehensive prediction visualizations\"\"\"\n",
        "        y_pred = metrics['predictions']\n",
        "        y_test = metrics['actuals']\n",
        "        errors = y_pred - y_test\n",
        "\n",
        "        fig = make_subplots(\n",
        "            rows=2, cols=2,\n",
        "            subplot_titles=(\n",
        "                'Predictions vs Actual RUL',\n",
        "                'Error Distribution',\n",
        "                'Scatter Plot',\n",
        "                'Cumulative Error'\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # 1. Predictions vs Actual\n",
        "        indices = np.arange(len(y_test))\n",
        "        fig.add_trace(\n",
        "            go.Scatter(x=indices, y=y_test, mode='markers',\n",
        "                      name='Actual RUL', marker=dict(color='blue', size=6)),\n",
        "            row=1, col=1\n",
        "        )\n",
        "        fig.add_trace(\n",
        "            go.Scatter(x=indices, y=y_pred, mode='markers',\n",
        "                      name='Predicted RUL', marker=dict(color='red', size=6)),\n",
        "            row=1, col=1\n",
        "        )\n",
        "\n",
        "        # 2. Error Distribution\n",
        "        fig.add_trace(\n",
        "            go.Histogram(x=errors, nbinsx=50, name='Error Distribution',\n",
        "                        marker_color='purple', showlegend=False),\n",
        "            row=1, col=2\n",
        "        )\n",
        "\n",
        "        # 3. Scatter Plot\n",
        "        min_val = min(y_test.min(), y_pred.min())\n",
        "        max_val = max(y_test.max(), y_pred.max())\n",
        "        fig.add_trace(\n",
        "            go.Scatter(x=y_test, y=y_pred, mode='markers',\n",
        "                      name='Predictions', marker=dict(color='green', size=6)),\n",
        "            row=2, col=1\n",
        "        )\n",
        "        fig.add_trace(\n",
        "            go.Scatter(x=[min_val, max_val], y=[min_val, max_val],\n",
        "                      mode='lines', name='Perfect Fit',\n",
        "                      line=dict(color='red', dash='dash')),\n",
        "            row=2, col=1\n",
        "        )\n",
        "\n",
        "        # 4. Cumulative Error\n",
        "        sorted_errors = np.sort(np.abs(errors))\n",
        "        cumulative = np.arange(1, len(sorted_errors) + 1) / len(sorted_errors) * 100\n",
        "        fig.add_trace(\n",
        "            go.Scatter(x=sorted_errors, y=cumulative, mode='lines',\n",
        "                      name='Cumulative Error', line=dict(color='orange', width=2)),\n",
        "            row=2, col=2\n",
        "        )\n",
        "\n",
        "        fig.update_layout(\n",
        "            height=800,\n",
        "            title_text=f\"{dataset_name} - Model Performance Analysis\",\n",
        "            showlegend=True\n",
        "        )\n",
        "\n",
        "        if save_path:\n",
        "            fig.write_html(save_path)\n",
        "\n",
        "        fig.show()\n",
        "\n",
        "        # Additional plots\n",
        "        fig2, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "        # Error by prediction magnitude\n",
        "        axes[0].scatter(y_pred, errors, alpha=0.5, s=30)\n",
        "        axes[0].axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
        "        axes[0].set_xlabel('Predicted RUL', fontsize=12)\n",
        "        axes[0].set_ylabel('Prediction Error', fontsize=12)\n",
        "        axes[0].set_title('Error vs Predicted RUL', fontsize=14, fontweight='bold')\n",
        "        axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "        # Absolute error distribution\n",
        "        abs_errors = np.abs(errors)\n",
        "        axes[1].hist(abs_errors, bins=50, color='coral', edgecolor='black', alpha=0.7)\n",
        "        axes[1].axvline(np.mean(abs_errors), color='red', linestyle='--',\n",
        "                       linewidth=2, label=f'Mean: {np.mean(abs_errors):.2f}')\n",
        "        axes[1].set_xlabel('Absolute Error', fontsize=12)\n",
        "        axes[1].set_ylabel('Frequency', fontsize=12)\n",
        "        axes[1].set_title('Absolute Error Distribution', fontsize=14, fontweight='bold')\n",
        "        axes[1].legend()\n",
        "        axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "        # QQ plot\n",
        "        from scipy import stats\n",
        "        stats.probplot(errors, dist=\"norm\", plot=axes[2])\n",
        "        axes[2].set_title('Q-Q Plot', fontsize=14, fontweight='bold')\n",
        "        axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        if save_path:\n",
        "            plt.savefig(save_path.replace('.html', '_additional.png'), dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "y9owCw0FhwKF"
      },
      "outputs": [],
      "source": [
        "# %% [markdown]\n",
        "# ## üöÄ Step 8: Complete Training Pipeline\n",
        "\n",
        "# %%\n",
        "def train_single_dataset(dataset_name, model_type='lstm', epochs=150):\n",
        "    \"\"\"Complete training pipeline for a single dataset\"\"\"\n",
        "\n",
        "    print(f\"\\n{'#'*80}\")\n",
        "    print(f\"{'#'*80}\")\n",
        "    print(f\"##  TRAINING PROGNOSAI MODEL: {dataset_name} - {model_type.upper()}\")\n",
        "    print(f\"{'#'*80}\")\n",
        "    print(f\"{'#'*80}\\n\")\n",
        "\n",
        "    # Initialize processor\n",
        "    processor = DataProcessor(dataset_name)\n",
        "\n",
        "    # Load data\n",
        "    print(\"üì• Loading datasets...\")\n",
        "    train_df, test_df, rul_df = processor.load_data()\n",
        "\n",
        "    # Preprocess training data\n",
        "    print(\"\\nüîß Preprocessing training data...\")\n",
        "    train_df = processor.add_features(train_df)\n",
        "    train_df = processor.compute_rul(train_df)\n",
        "    train_df = processor.select_features(train_df)\n",
        "\n",
        "    # Preprocess test data\n",
        "    print(\"üîß Preprocessing test data...\")\n",
        "    test_df = processor.add_features(test_df)\n",
        "    test_df = processor.select_features(test_df)\n",
        "\n",
        "    # Normalize\n",
        "    print(\"üìä Normalizing features...\")\n",
        "    train_df, test_df, feature_cols = processor.normalize_features(train_df, test_df)\n",
        "\n",
        "    print(f\"   Selected {len(feature_cols)} features\")\n",
        "\n",
        "    # Create sequences\n",
        "    print(\"\\nüîÑ Creating sequences...\")\n",
        "    X_train, y_train = processor.create_sequences(train_df, is_test=False)\n",
        "    X_test = processor.create_sequences(test_df, is_test=True)\n",
        "    y_test = rul_df['RUL'].values\n",
        "    y_test = np.clip(y_test, 0, processor.config['rul_clip'])\n",
        "\n",
        "    # Split training data\n",
        "    print(\"‚úÇÔ∏è Splitting training/validation sets...\")\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_train, y_train, test_size=0.2, random_state=SEED, shuffle=True\n",
        "    )\n",
        "\n",
        "    print(f\"   Training: {X_train.shape}\")\n",
        "    print(f\"   Validation: {X_val.shape}\")\n",
        "    print(f\"   Test: {X_test.shape}\")\n",
        "\n",
        "    # Create model\n",
        "    print(f\"\\nü§ñ Creating {model_type.upper()} model...\")\n",
        "    input_shape = (X_train.shape[1], X_train.shape[2])\n",
        "\n",
        "    if model_type.lower() == 'lstm':\n",
        "        model = create_lstm_model(input_shape)\n",
        "    elif model_type.lower() == 'gru':\n",
        "        model = create_gru_model(input_shape)\n",
        "    elif model_type.lower() == 'cnn_lstm':\n",
        "        model = create_cnn_lstm_model(input_shape)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown model type: {model_type}\")\n",
        "\n",
        "    print(f\"\\nüìã Model Summary:\")\n",
        "    model.summary()\n",
        "\n",
        "    # Train model\n",
        "    trainer = ModelTrainer(model, dataset_name, model_type)\n",
        "    history = trainer.train(\n",
        "        X_train, y_train,\n",
        "        X_val, y_val,\n",
        "        epochs=epochs,\n",
        "        batch_size=processor.config['batch_size']\n",
        "    )\n",
        "\n",
        "    # Plot training history\n",
        "    print(\"\\nüìà Plotting training history...\")\n",
        "    trainer.plot_history()\n",
        "\n",
        "    # Evaluate on test set\n",
        "    print(\"\\nüéØ Evaluating on test set...\")\n",
        "    evaluator = ModelEvaluator()\n",
        "    metrics = evaluator.evaluate_model(model, X_test, y_test)\n",
        "    evaluator.print_metrics(metrics, dataset_name)\n",
        "\n",
        "    # Visualize predictions\n",
        "    print(\"üìä Creating prediction visualizations...\")\n",
        "    evaluator.plot_predictions(\n",
        "        metrics,\n",
        "        dataset_name,\n",
        "        save_path=str(trainer.save_dir / 'predictions.html')\n",
        "    )\n",
        "\n",
        "    # Save artifacts\n",
        "    print(\"\\nüíæ Saving artifacts...\")\n",
        "\n",
        "    # Save scaler\n",
        "    scaler_path = trainer.save_dir / 'scaler.pkl'\n",
        "    with open(scaler_path, 'wb') as f:\n",
        "        pickle.dump(processor.scaler, f)\n",
        "    print(f\"   ‚úì Scaler saved: {scaler_path}\")\n",
        "\n",
        "    # Save configuration\n",
        "    config = {\n",
        "        'dataset_name': dataset_name,\n",
        "        'model_type': model_type,\n",
        "        'seq_len': processor.config['seq_len'],\n",
        "        'rul_clip': processor.config['rul_clip'],\n",
        "        'feature_cols': feature_cols,\n",
        "        'input_shape': input_shape,\n",
        "        'metrics': {\n",
        "            'rmse': float(metrics['rmse']),\n",
        "            'mae': float(metrics['mae']),\n",
        "            'r2': float(metrics['r2'])\n",
        "        },\n",
        "        'training_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "    }\n",
        "\n",
        "    config_path = trainer.save_dir / 'config.json'\n",
        "    with open(config_path, 'w') as f:\n",
        "        json.dump(config, f, indent=4)\n",
        "    print(f\"   ‚úì Config saved: {config_path}\")\n",
        "\n",
        "    # Save predictions\n",
        "    predictions_df = pd.DataFrame({\n",
        "        'unit_id': range(len(y_test)),\n",
        "        'actual_rul': y_test,\n",
        "        'predicted_rul': metrics['predictions'],\n",
        "        'error': metrics['predictions'] - y_test,\n",
        "        'abs_error': np.abs(metrics['predictions'] - y_test)\n",
        "    })\n",
        "    predictions_path = trainer.save_dir / 'predictions.csv'\n",
        "    predictions_df.to_csv(predictions_path, index=False)\n",
        "    print(f\"   ‚úì Predictions saved: {predictions_path}\")\n",
        "\n",
        "    print(f\"\\n‚úÖ Training complete for {dataset_name}!\")\n",
        "    print(f\"üìÅ All artifacts saved to: {trainer.save_dir}\\n\")\n",
        "\n",
        "    return {\n",
        "        'model': model,\n",
        "        'scaler': processor.scaler,\n",
        "        'config': config,\n",
        "        'metrics': metrics,\n",
        "        'save_dir': trainer.save_dir\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "oq2-olyNhwNg"
      },
      "outputs": [],
      "source": [
        "# %% [markdown]\n",
        "# ## üéØ Step 9: Train All Available Datasets\n",
        "\n",
        "# %%\n",
        "def train_all_available_datasets(model_type='lstm', epochs=150):\n",
        "    \"\"\"Train models for all available datasets\"\"\"\n",
        "\n",
        "    if not available_datasets:\n",
        "        print(\"‚ùå No datasets available. Please upload datasets first.\")\n",
        "        return {}\n",
        "\n",
        "    print(f\"\\nüöÄ Starting training for {len(available_datasets)} dataset(s)\")\n",
        "    print(f\"   Model type: {model_type.upper()}\")\n",
        "    print(f\"   Epochs: {epochs}\")\n",
        "    print(f\"   Datasets: {', '.join(available_datasets)}\\n\")\n",
        "\n",
        "    results = {}\n",
        "    summary_data = []\n",
        "\n",
        "    for dataset in available_datasets:\n",
        "        try:\n",
        "            result = train_single_dataset(dataset, model_type, epochs)\n",
        "            results[dataset] = result\n",
        "\n",
        "            # Collect summary\n",
        "            summary_data.append({\n",
        "                'Dataset': dataset,\n",
        "                'Model': model_type.upper(),\n",
        "                'RMSE': result['metrics']['rmse'],\n",
        "                'MAE': result['metrics']['mae'],\n",
        "                'R¬≤': result['metrics']['r2'],\n",
        "                'Save Dir': str(result['save_dir'])\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\n‚ùå Error training {dataset}: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            continue\n",
        "\n",
        "    # Generate comparison report\n",
        "    if results:\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(\"üìä TRAINING SUMMARY - ALL DATASETS\")\n",
        "        print(f\"{'='*80}\\n\")\n",
        "\n",
        "        summary_df = pd.DataFrame(summary_data)\n",
        "        print(summary_df.to_string(index=False))\n",
        "\n",
        "        # Save summary\n",
        "        summary_path = Path('./models/training_summary.csv')\n",
        "        summary_df.to_csv(summary_path, index=False)\n",
        "        print(f\"\\n‚úì Summary saved to: {summary_path}\")\n",
        "\n",
        "        # Create comparison visualization\n",
        "        fig = go.Figure()\n",
        "\n",
        "        datasets_list = list(results.keys())\n",
        "        rmse_values = [results[d]['metrics']['rmse'] for d in datasets_list]\n",
        "        mae_values = [results[d]['metrics']['mae'] for d in datasets_list]\n",
        "\n",
        "        fig.add_trace(go.Bar(\n",
        "            name='RMSE',\n",
        "            x=datasets_list,\n",
        "            y=rmse_values,\n",
        "            marker_color='steelblue'\n",
        "        ))\n",
        "\n",
        "        fig.add_trace(go.Bar(\n",
        "            name='MAE',\n",
        "            x=datasets_list,\n",
        "            y=mae_values,\n",
        "            marker_color='coral'\n",
        "        ))\n",
        "\n",
        "        fig.update_layout(\n",
        "            title='Model Performance Comparison Across Datasets',\n",
        "            xaxis_title='Dataset',\n",
        "            yaxis_title='Error (cycles)',\n",
        "            barmode='group',\n",
        "            template='plotly_white',\n",
        "            height=500\n",
        "        )\n",
        "\n",
        "        comparison_path = Path('./models/performance_comparison.html')\n",
        "        fig.write_html(comparison_path)\n",
        "        fig.show()\n",
        "        print(f\"\\n‚úì Comparison chart saved to: {comparison_path}\")\n",
        "\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(\"‚úÖ ALL TRAINING COMPLETE!\")\n",
        "        print(f\"{'='*80}\\n\")\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lJelkCxtk1Ux"
      },
      "outputs": [],
      "source": [
        "# %% [markdown]\n",
        "# ## üîÆ Step 10: Inference Engine (Deployment Ready)\n",
        "\n",
        "# %%\n",
        "class PrognosAIInference:\n",
        "    \"\"\"Production-ready inference engine\"\"\"\n",
        "\n",
        "    def __init__(self, model_path, scaler_path, config_path):\n",
        "        \"\"\"\n",
        "        Initialize inference engine\n",
        "\n",
        "        Args:\n",
        "            model_path: Path to saved Keras model (.keras file)\n",
        "            scaler_path: Path to saved scaler (.pkl file)\n",
        "            config_path: Path to config JSON file\n",
        "        \"\"\"\n",
        "        # Load configuration\n",
        "        with open(config_path, 'r') as f:\n",
        "            self.config = json.load(f)\n",
        "\n",
        "        # Load scaler\n",
        "        with open(scaler_path, 'rb') as f:\n",
        "            self.scaler = pickle.load(f)\n",
        "\n",
        "        # Load model\n",
        "        self.model = keras.models.load_model(model_path, safe_mode=False)\n",
        "\n",
        "        self.seq_len = self.config['seq_len']\n",
        "        self.rul_clip = self.config['rul_clip']\n",
        "        self.feature_cols = self.config['feature_cols']\n",
        "\n",
        "        print(f\"‚úÖ Inference engine initialized\")\n",
        "        print(f\"   Dataset: {self.config['dataset_name']}\")\n",
        "        print(f\"   Model: {self.config['model_type']}\")\n",
        "        print(f\"   Sequence length: {self.seq_len}\")\n",
        "        print(f\"   Features: {len(self.feature_cols)}\")\n",
        "\n",
        "    def predict_rul(self, sensor_data):\n",
        "        \"\"\"\n",
        "        Predict RUL for sensor data\n",
        "\n",
        "        Args:\n",
        "            sensor_data: DataFrame or numpy array with sensor readings\n",
        "                        Shape: (n_timesteps, n_features)\n",
        "\n",
        "        Returns:\n",
        "            Predicted RUL value\n",
        "        \"\"\"\n",
        "        # Convert to numpy if DataFrame\n",
        "        if isinstance(sensor_data, pd.DataFrame):\n",
        "            sensor_data = sensor_data[self.feature_cols].values\n",
        "\n",
        "        # Scale data\n",
        "        sensor_scaled = self.scaler.transform(sensor_data)\n",
        "\n",
        "        # Create sequence\n",
        "        if len(sensor_scaled) < self.seq_len:\n",
        "            # Pad if too short\n",
        "            pad_len = self.seq_len - len(sensor_scaled)\n",
        "            pad = np.repeat(sensor_scaled[0:1], pad_len, axis=0)\n",
        "            sequence = np.vstack([pad, sensor_scaled])\n",
        "        else:\n",
        "            # Take last seq_len timesteps\n",
        "            sequence = sensor_scaled[-self.seq_len:]\n",
        "\n",
        "        # Predict\n",
        "        sequence = sequence.reshape(1, self.seq_len, -1)\n",
        "        prediction = self.model.predict(sequence, verbose=0)[0][0]\n",
        "\n",
        "        # Clip to valid range\n",
        "        prediction = np.clip(prediction, 0, self.rul_clip)\n",
        "\n",
        "        return float(prediction)\n",
        "\n",
        "    def predict_batch(self, sensor_data_list):\n",
        "        \"\"\"\n",
        "        Predict RUL for multiple engines\n",
        "\n",
        "        Args:\n",
        "            sensor_data_list: List of DataFrames or numpy arrays\n",
        "\n",
        "        Returns:\n",
        "            Array of predicted RUL values\n",
        "        \"\"\"\n",
        "        sequences = []\n",
        "\n",
        "        for data in sensor_data_list:\n",
        "            if isinstance(data, pd.DataFrame):\n",
        "                data = data[self.feature_cols].values\n",
        "\n",
        "            data_scaled = self.scaler.transform(data)\n",
        "\n",
        "            if len(data_scaled) < self.seq_len:\n",
        "                pad_len = self.seq_len - len(data_scaled)\n",
        "                pad = np.repeat(data_scaled[0:1], pad_len, axis=0)\n",
        "                sequence = np.vstack([pad, data_scaled])\n",
        "            else:\n",
        "                sequence = data_scaled[-self.seq_len:]\n",
        "\n",
        "            sequences.append(sequence)\n",
        "\n",
        "        sequences = np.array(sequences)\n",
        "        predictions = self.model.predict(sequences, verbose=0).flatten()\n",
        "        predictions = np.clip(predictions, 0, self.rul_clip)\n",
        "\n",
        "        return predictions\n",
        "\n",
        "    def get_risk_level(self, rul):\n",
        "        \"\"\"\n",
        "        Determine risk level based on RUL\n",
        "\n",
        "        Args:\n",
        "            rul: Remaining Useful Life value\n",
        "\n",
        "        Returns:\n",
        "            Risk level and recommendation\n",
        "        \"\"\"\n",
        "        if rul > 50:\n",
        "            return {\n",
        "                'level': 'LOW',\n",
        "                'color': 'green',\n",
        "                'action': 'Normal operation',\n",
        "                'priority': 1\n",
        "            }\n",
        "        elif rul > 25:\n",
        "            return {\n",
        "                'level': 'MEDIUM',\n",
        "                'color': 'yellow',\n",
        "                'action': 'Schedule maintenance',\n",
        "                'priority': 2\n",
        "            }\n",
        "        elif rul > 10:\n",
        "            return {\n",
        "                'level': 'HIGH',\n",
        "                'color': 'orange',\n",
        "                'action': 'Plan immediate maintenance',\n",
        "                'priority': 3\n",
        "            }\n",
        "        else:\n",
        "            return {\n",
        "                'level': 'CRITICAL',\n",
        "                'color': 'red',\n",
        "                'action': 'URGENT: Immediate maintenance required',\n",
        "                'priority': 4\n",
        "            }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xU3KMPMnk-Sn"
      },
      "outputs": [],
      "source": [
        "# %% [markdown]\n",
        "# ## üéÆ Step 11: MAIN EXECUTION - Train Selected Datasets\n",
        "\n",
        "# %%\n",
        "# @title üöÄ **RUN THIS CELL TO TRAIN MODELS**\n",
        "\n",
        "# ============================================\n",
        "# CONFIGURATION\n",
        "# ============================================\n",
        "\n",
        "# Select which datasets to train\n",
        "DATASETS_TO_TRAIN = available_datasets  # Train all available datasets\n",
        "# Or specify manually: DATASETS_TO_TRAIN = ['FD001', 'FD003']\n",
        "\n",
        "# Select model type\n",
        "MODEL_TYPE = 'lstm'  # Options: 'lstm', 'gru', 'cnn_lstm'\n",
        "\n",
        "# Training epochs\n",
        "EPOCHS = 150  # Recommended: 100-200\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OhzspxKGrl3B"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "keras.config.enable_unsafe_deserialization()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zP0ewsB5k-Vm"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# EXECUTION\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üöÄ PROGNOSAI TRAINING PIPELINE\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nüìã Configuration:\")\n",
        "print(f\"   Datasets to train: {DATASETS_TO_TRAIN}\")\n",
        "print(f\"   Model type: {MODEL_TYPE.upper()}\")\n",
        "print(f\"   Training epochs: {EPOCHS}\")\n",
        "print(f\"   GPU available: {len(tf.config.list_physical_devices('GPU')) > 0}\")\n",
        "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "# Train all selected datasets\n",
        "if DATASETS_TO_TRAIN:\n",
        "    training_results = train_all_available_datasets(\n",
        "        model_type=MODEL_TYPE,\n",
        "        epochs=EPOCHS\n",
        "    )\n",
        "\n",
        "    # Download results\n",
        "    if training_results:\n",
        "        print(\"\\nüì¶ Creating downloadable archive...\")\n",
        "        !zip -r prognosai_models.zip ./models\n",
        "\n",
        "        print(\"\\nüì• Download your trained models:\")\n",
        "        files.download('prognosai_models.zip')\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"‚úÖ TRAINING COMPLETE! All models and artifacts are ready.\")\n",
        "        print(\"=\"*80)\n",
        "        print(\"\\nüìÇ Saved artifacts include:\")\n",
        "        print(\"   ‚úì Trained Keras models (.keras)\")\n",
        "        print(\"   ‚úì Scalers (.pkl)\")\n",
        "        print(\"   ‚úì Configurations (.json)\")\n",
        "        print(\"   ‚úì Training logs (.csv)\")\n",
        "        print(\"   ‚úì Predictions (.csv)\")\n",
        "        print(\"   ‚úì Visualizations (.html, .png)\")\n",
        "else:\n",
        "    print(\"‚ùå No datasets available for training!\")\n",
        "    print(\"Please run the upload cell first to upload your datasets.\")\n",
        "\n",
        "# %% [markdown]\n",
        "# ## üîç Step 12: Test Inference Engine (Example)\n",
        "\n",
        "# %%\n",
        "# @title üß™ **Test Inference Engine** (Run after training)\n",
        "\n",
        "# Select a trained model to test\n",
        "if training_results:\n",
        "    # Get first available model\n",
        "    test_dataset = list(training_results.keys())[0]\n",
        "    result = training_results[test_dataset]\n",
        "\n",
        "    print(f\"üß™ Testing inference engine with {test_dataset} model\\n\")\n",
        "\n",
        "    # Initialize inference engine\n",
        "    inference = PrognosAIInference(\n",
        "        model_path=str(result['save_dir'] / 'best_model.keras'),\n",
        "        scaler_path=str(result['save_dir'] / 'scaler.pkl'),\n",
        "        config_path=str(result['save_dir'] / 'config.json')\n",
        "    )\n",
        "\n",
        "    # Load test data for demonstration\n",
        "    processor = DataProcessor(test_dataset)\n",
        "    train_df, test_df, rul_df = processor.load_data()\n",
        "    test_df = processor.add_features(test_df)\n",
        "    test_df = processor.select_features(test_df)\n",
        "\n",
        "    # Get first test unit\n",
        "    first_unit = test_df[test_df['unit'] == 1]\n",
        "    feature_cols = [col for col in first_unit.columns\n",
        "                   if col not in ['unit', 'cycle']]\n",
        "\n",
        "    # Predict RUL\n",
        "    predicted_rul = inference.predict_rul(first_unit[feature_cols])\n",
        "    actual_rul = rul_df.iloc[0]['RUL']\n",
        "\n",
        "    print(f\"\\nüìä Prediction Results for Unit 1:\")\n",
        "    print(f\"   Predicted RUL: {predicted_rul:.2f} cycles\")\n",
        "    print(f\"   Actual RUL: {actual_rul:.2f} cycles\")\n",
        "    print(f\"   Error: {abs(predicted_rul - actual_rul):.2f} cycles\")\n",
        "\n",
        "    # Get risk assessment\n",
        "    risk = inference.get_risk_level(predicted_rul)\n",
        "    print(f\"\\n‚ö†Ô∏è Risk Assessment:\")\n",
        "    print(f\"   Level: {risk['level']}\")\n",
        "    print(f\"   Action: {risk['action']}\")\n",
        "    print(f\"   Priority: {risk['priority']}\")\n",
        "\n",
        "    print(\"\\n‚úÖ Inference engine test complete!\")\n",
        "else:\n",
        "    print(\"‚ùå No trained models available. Please train models first.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "H5gHsjprk-Ye"
      },
      "outputs": [],
      "source": [
        "# %% [markdown]\n",
        "# ## üìñ Usage Instructions\n",
        "\n",
        "# %%\n",
        "# @markdown ### **üìñ HOW TO USE THIS NOTEBOOK:**\n",
        "#\n",
        "# **Step 1: Upload Datasets**\n",
        "# - Run the upload cell in Step 3\n",
        "# - Upload your CMAPSS .txt files or a ZIP archive\n",
        "# - Verify that files are detected correctly\n",
        "#\n",
        "# **Step 2: Configure Training**\n",
        "# - In Step 11, modify the configuration:\n",
        "#   - `DATASETS_TO_TRAIN`: Select which datasets to train\n",
        "#   - `MODEL_TYPE`: Choose 'lstm', 'gru', or 'cnn_lstm'\n",
        "#   - `EPOCHS`: Set training epochs (150 recommended)\n",
        "#\n",
        "# **Step 3: Run Training**\n",
        "# - Click the play button in Step 11\n",
        "# - Monitor training progress (loss, RMSE, MAE)\n",
        "# - Wait for completion (may take 15-60 minutes per dataset)\n",
        "#\n",
        "# **Step 4: Download Results**\n",
        "# - After training, models.zip will be created automatically\n",
        "# - Download contains:\n",
        "#   - Trained models (.keras)\n",
        "#   - Scalers (.pkl)\n",
        "#   - Configurations (.json)\n",
        "#   - Predictions and visualizations"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}